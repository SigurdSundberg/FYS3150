\documentclass[10pt, a4paper]{amsart}
% \documentclass[10pt,showpacs,preprintnumbers,footinbib,amsmath,amssymb,aps,prl,twocolumn,groupedaddress,superscriptaddress,showkeys]{revtex4-1}
\usepackage[]{graphicx}
\usepackage[]{hyperref}
\usepackage[]{physics}
\usepackage[]{listings}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage[ruled,vlined]{algorithm2e}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=c++,                    % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                     % sets default tabsize to 2 spaces
}


\title[Solving the Poisson-equation in one dimension]{Solving the Poisson-equation in one dimension: \\
\normalsize{Tridiagonal Matrix Algorithm\\
 and \\
 LU-decomposition} \\
  \hrulefill\small{ FYS3150: Computational Physics }\hrulefill}

\author[Sundberg]{Sigurd Sandvoll Sundberg \\
  \href{https://github.com/SigurdSundberg/FYS3150/tree/master/project1}{\texttt{github.com/sigurdsundberg}}}

\begin{document}

\begin{titlepage}
\begin{abstract}
Abstract write last.
\end{abstract}
\maketitle
\tableofcontents
\end{titlepage}

\section{Introduction}
The use of differential equation has a sentral role in every branch of science. Linear second-order differential equations cover many of the important differential equations. Many of which can be manipulated to be solved as a linear algebra problem, for which we can develop algorithms to solve to problems numerically. There are limitations when it comes to numerically solutions. If the number of floating point operations(FLOPs) becomes too big the calculations can get exceedingly slow. One can also risk numerical inaccuracies simply due to a high amount of FLOPs. This puts the emphasis on developing algorithms which are stable, accurate and quick. 
In this project we will take a look at solving the one-dimensional Poisson equation with Dirichlet boundary conditions by rewriting it as a set of linear equations. Excplicitly we want to solve the following equation
\begin{equation}
-u''(x) = f(x), \quad x\in(0,1), \quad u(0)=u(1)=0.\label{eq:base}
\end{equation}
We will device an algorithm for a tridiagonal matrix, both the general case and a specialized case. Including, we will take a look at the more general method of solving matrix equations using LU-decomposition, comparing run times and space between the different algorithms. In addition to looking at relative error and finding the mesh-grid which gives the best approximation. Lastly, the findings will be discussed. 


\section{Theory}
\subsection{Poisson equation}
A classic exmaple of linear second-order differential equation is the Poisson's equation from electromagnetism. The electrostatic potential $\Phi$ is generated by localized charge distribution $\rho(\mathbf{r}.$ The equation read 
\begin{equation}
	\nabla^2\Phi = -4\pi\rho(\mathbf{r})\nonumber.
\end{equation}
If we have spherical symmetric $\Phi$ and $\rho(\mathbf{r}$ the equations simplifies to one-dimensional equation in $r$, namely 
\begin{equation}
	\frac{1}{r^2}\frac{d}{dr}\left( r^2\frac{d\Phi}{dr}\right) = -4\pi\rho(r).	\nonumber
\end{equation}
By substituting $\Phi(r) = \phi(r)/r$ we get 
\begin{equation}
	\frac{d^2\phi}{dr^2} = -4\pi r\rho(r).\nonumber
\end{equation}
This is a linear second-order differential equation on the form in equation \ref{eq:base} with $\phi \rightarrow u$, $r \rightarrow x$ and $f(x) = 4\pi r\rho(r)$. We are left with
\begin{equation}
	-u''(x) = f(x)\nonumber
\end{equation}

In our study the source term will be $f(x) = 100e^{-10x}$ and and the analytical solution is given by $u(x) = 1-(1-e^{-10})x-e^{-10x}$, this can be verified easily by taking the second derivative of $u(x)$.
\subsection{Approximating the second derivative}
% >> Going from diff to linear form 
To solve one-dimensional Poisson equiation numerically we will need to discretize the equation. Instead of having continous functions, we get 
\begin{equation}\label{eq:disc}
	\begin{split}
		x \rightarrow x_i\in[x_0,x_1, \dots ,x_i,\dots ,x_{n+1}]\\
		u(x) \rightarrow u(x_i) = u_i \in[u_0,u_1, \dots ,u_i,\dots ,u_{n+1}]\\	
		f(x) \rightarrow f(x_i) = f_i \in[f_0,f_1, \dots ,f_i,\dots ,f_{n+1}],
	\end{split}
\end{equation}
using grid-points $x_i = ih$ in the interval from $x_0 = 0$ to $x_{n+1} = 1$ since we have an open interval. The step length h is defined as $h = 1/(n+1)$. We then have the boundary conditions $x_0 = v_{n+1} = 0$. The approximation for the second derivative we need from \ref{eq:base}, can be found through Taylor expansion of $u(x\pm h)$ around $x$.
\begin{equation}
	u(x\pm h) = u(x) \pm hu'(x) + \frac{h^2u''(x)}{2!} \pm \frac{h^3u'''(x)}{3!} + \mathcal{O}(h^4)\label{eq:taylor_exp}
\end{equation}
If we look at the two equation we get from $u(x+h)$ and $u(x-h)$, we can see that the first and third derivatives cancel eachother out when we add the two equations together. If we solve the resulting equation for $u''(x)$ we get 
\begin{equation}
u''(x) = \frac{u(x+h) + u(x-h) - 2u(x)}{h^2} + \mathcal{O}(h^2).\label{eq:second_der}
\end{equation}
Using our discretization of the continous functions given by equation \ref{eq:disc}, we have the following equation
\begin{equation}
-u''(x) \simeq f_i = \frac{u_{i+1} + u_{i-1} -2u_i}{h^2} +\mathcal{O}(h^2). \label{eq:inserted_for_disc}
\end{equation}
for $i = 1,\cdots , n$. 
If we define $f^*_i = h^2f_i$, equation \ref{eq:inserted_for_disc} becomes $-u_{i+1} - u_{i-1} +2u_i = f^*_i$. If we try inserting values of $i= 1, 2, 3, 4,\dots , n-1 n$  we get the following
\begin{equation}\label{eq:disc_ins_i}
	\begin{split}
		-u_2 - u_0 + 2u_1 = f^*_1 \quad i = 1\\
		-u_3 - u_1 + 2u_2 = f^*_2 \quad i = 2\\
		-u_4 - u_2 + 2u_3 = f^*_3 \quad i = 3\\
		-u_5 - u_3 + 2u_4 = f^*_4 \quad i = 4\\
		\vdots \\
		-u_n - u_{n-2} + 2u_{n-1} = f^*_{n-1} \quad i = n-1\\
		-u_{n+1} - u_{n-1} + 2u_n = f^*_n \quad i = n
	\end{split}
\end{equation}
With the Dirichlet boundary conditions $u(0) = u(n+1) = 0$ we see resembles a matrix with $2$ along the diagonal and $-1$ directly above and below the leading diagonal. We see that we can solve as a linear algebra problem, where we want to find $\vec{x}$, on the following form. 
\begin{equation}
\mathbf{A}\vec{x} = f^*, \nonumber
\end{equation}
where $\mathbf{A}$ is an $n\times n$ tridiagonal matrix which we found in \ref{eq:disc_ins_i}.
\begin{equation}\label{mat:tridiag_1_n}
\mathbf{A} = 
\begin{bmatrix}
2 & -1 & 0 & \dots & \dots & 0 \\
-1 & 2 & -1 & 0 & \dots & \dots \\
0 & -1 & 2 & -1 & 0 & \dots \\
\dots & \dots & \ddots & \ddots & \ddots & \dots\\
0 & \dots & 0 & -1 & 2 & -1 \\
0 & \dots & \dots & 0 & -1 & 2
\end{bmatrix}
\end{equation}

\section{Algorithms} 
For all programs, benchmark calulations and plots, see: \\
\href{https://github.com/SigurdSundberg/FYS3150/tree/master/project1}{\textbf{github.com/SigurdSundberg/FYS3150}}


\subsection{Tridiagonal Matrix Algorithm}
Looking at a general case of solving a tridiagonal matrix on the form $\mathbf{A}\vec{x} = f^*$ we have 
\begin{equation} 
	\begin{bmatrix}
		b_1 & c_1 & 0 &\dots &\dots &\dots \\
		a_1 & b_2 & c_2 & 0 &\dots &\dots\\
		0 & a_2 & b_3 & c_3 & 0 &\dots \\
		\dots &\dots &\dots &\dots &\dots &\dots\\
		\dots &\dots &\dots & a_{n-2} & b_{n-1} & c_{n-1} \\
		\dots &\dots &\dots &\dots &a_{n-1} & b_n
	\end{bmatrix}
	\begin{bmatrix}
		x_1 \\
		x_2 \\
		\dots \\
		\dots \\
		\dots \\
		x_n
	\end{bmatrix} 
	= 
	\begin{bmatrix}
		f^*_1\\
		f^*_2\\
		\dots \\
		\dots \\
		\dots \\
		f^*_n
	\end{bmatrix}
\end{equation}

Let's take general case for a $4\times 4$ tridiagonal matrix and use Gaussian elimination to see if we can spot a pattern.
Our system can then be written 
\begin{align}
	\mathbf{A}\vec{x} &= f^*\nonumber\\
	\begin{bmatrix}
		b_1 & c_1 & 0 & 0\\
		a_1 & b_2 & c_2 & 0\\
		0 & a_2 & b_3 & c_3 \\
		0 & 0 & a_3 & b_4 
	\end{bmatrix}
	\begin{bmatrix}
		x_1\\
		x_2\\
		x_3\\
		x_4
	\end{bmatrix}
	&= 
	\begin{bmatrix}
		f^*_1\\
		f^*_2\\
		f^*_3\\
		f^*_4
	\end{bmatrix}
\end{align}

The set of equations we will use Gaussian elimination on is therefore 
\begin{equation}
 \left[
  \begin{array}{cccc|c}
	b_1 & c_1 & 0 & 0 & f^*_1\\
	a_1 & b_2 & c_2 & 0 & f^*_2\\
	0 & a_2 & b_3 & c_3 & f^*_3\\
	0 & 0 & a_3 & b_4 & f^*_4
  \end{array}
 \right]
\end{equation}
First we will use forward substitution to remove all the elements along the diagonal below the leading diagonal. We start with $\textrm{II} - \textrm{I}(a_1/b_1)$ to remove $a_1$ from $\textrm{II}$, where \textrm{I} refers to row one, \textrm{II} refers to row two and so on. We now how 
\begin{equation}
	\left[
  \begin{array}{cccc|c}
	b_1 & c_1 & 0 & 0 & f^*_1\\
	0 & b_2- c_1a_1/b_1 & c_2 & 0 & f^*_2 - f^*_1a_1/b_1\\
	0 & a_2 & b_3 & c_3 & f^*_3\\
	0 & 0 & a_3 & b_4 & f^*_4
  \end{array}
 \right]
\end{equation}
We will define two new variables $\tilde{b}_2 =b_2- c_1a_1/b_1 $ and $\tilde{f}_2 = f^*_2 - f^*_1a_1/b_1$ to simplify the matrix and continue the forward substitution with $\textrm{III} - \textrm{II}(a_2/\tilde{b}_2)$.
\begin{equation}
 \left[
  \begin{array}{cccc|c}
	b_1 & c_1 & 0 & 0 & f^*_1\\
	0 & \tilde{b}_2 & c_2 & 0 & \tilde{f}_2\\
	0 & 0 & b_3-c_2a_2/\tilde{b}_2 & c_3 & f^*_3-\tilde{f}_2a_2/\tilde{b}_2\\
	0 & 0 & a_3 & b_4 & f^*_4
  \end{array}
 \right]
\end{equation}
Using the same definition as above as prior $\tilde{b}_i = b_i -c_{i-1}a_{i-1}/b_{i-1}$ and $\tilde{f}_i = f_i - f_{i-1}a_{i-1}/b_{i-1}$ we are left with 
\begin{equation}
\left[
  \begin{array}{cccc|c}
	b_1 & c_1 & 0 & 0 & f^*_1\\
	0 & \tilde{b}_2 & c_2 & 0 & \tilde{f}_2\\
	0 & 0 & \tilde{b}_3 & c_3 & \tilde{f}_3\\
	0 & 0 & a_3 & b_4 & f^*_4
  \end{array}
 \right]
\end{equation}
Repeating this one more time we can eliminate $a_3$. We see however that we see that a pattern emerges for the forward substitution namely 
\begin{equation}\label{algo:forward_sub}
\begin{split}
	\tilde{b}_i = b_i -\frac{c_{i-1}a_{i-1}}{\tilde{b}_{i-1}} \\
	\tilde{f}_i = f^*_i - \frac{\tilde{f}_{i-1}a_{i-1}}{\tilde{b}_{i-1}}.
\end{split}
\end{equation}
If we define $\tilde{b}_1 = b_1$ and $\tilde{f}_1 = f_1$ and get the follwing matrix 
\begin{equation}
\left[
  \begin{array}{cccc|c}
	\tilde{b}_1 & c_1 & 0 & 0 & \tilde{f}_1\\
	0 & \tilde{b}_2 & c_2 & 0 & \tilde{f}_2\\
	0 & 0 & \tilde{b}_3 & c_3 & \tilde{f}_3\\
	0 & 0 & 0 & \tilde{b}_4 & \tilde{f}_4
  \end{array}
 \right]
\end{equation}
We can now solve for $\vec{x}$ by doing a backwards substitution. We find $x_4$ by doing $\tilde{f}_4/\tilde{b}_4$, giving us 
\begin{equation}\label{sub:back}
\left[
  \begin{array}{cccc|c}
	\tilde{b}_1 & c_1 & 0 & 0 & \tilde{f}_1\\
	0 & \tilde{b}_2 & c_2 & 0 & \tilde{f}_2\\
	0 & 0 & \tilde{b}_3 & c_3 & \tilde{f}_3\\
	0 & 0 & 0 & 1 & x_4
  \end{array}
 \right]
\end{equation}
Doing a backwards substitution on matrix \ref{sub:back} by taking $\textrm{III} - \textrm{IV}(c_3)$ and dividing \textrm{III} by $\tilde{b}_3$ to normalize the row. We then define $x_3 = (\tilde{f}_3 - x_4c_3)/\tilde{b}_3$, which gives us 
\begin{equation}
\left[
  \begin{array}{cccc|c}
	\tilde{b}_1 & c_1 & 0 & 0 & \tilde{f}_1\\
	0 & \tilde{b}_2 & c_2 & 0 & \tilde{f}_2\\
	0 & 0 & 1 & 0 & x_3\\
	0 & 0 & 0 & 1 & x_4
  \end{array}
 \right]
\end{equation}
Repeating the process on row \textrm{II} and using the same definition as above we get
\begin{equation}
\left[
  \begin{array}{cccc|c}
	\tilde{b}_1 & c_1 & 0 & 0 & \tilde{f}_1\\
	0 & 1 & 0 & 0 & x_2\\
	0 & 0 & 1 & 0 & x_3\\
	0 & 0 & 0 & 1 & x_4
  \end{array}
 \right]
\end{equation}
we see that a pattern emerges, namely
\begin{equation}\label{algo:backwards_sub}
	x_i = \frac{\tilde{f}_i - x_{i+1}c_i}{\tilde{b}_i}
\end{equation}
which we will call backwards substitution from now. 

Assuming we have pre initialized vectors $a, b, c, f^*$ and defining $\tilde{b}_1 = b_1$ and $\tilde{f}_1 = 1$, our general algorithm from respectivly \ref{algo:forward_sub} and \ref{algo:backwards_sub} will read as follows
\begin{algorithm}[H]\label{psudo:for}
\SetAlgoLined
 initialization\;
 \For{i = 2, 3, \dots , n}{
  $quotient = a_{i-1}/\tilde{â€¢}tilde{b}_{i-1}$\;
  $\tilde{b}_i = b_i - c_{i-1}\cdot quotient$\;
  $\tilde{f}_i = f^*_i - \tilde{f}_{i-1}\cdot quotient$\;  
 }
\caption{Forward substitution}
\end{algorithm}
\begin{algorithm}[H]\label{psudo:back}
\SetAlgoLined
 $x_n = \tilde{f}_n/\tilde{b}_n$\;
 \For{i = n-1, n-2,\dots , 1}{
	$x_i = (\tilde{f}_i - x_{i+1}c_i)/\tilde{b}_i$\;  
 }
\caption{Backward substitution}
\end{algorithm}

We are also interested in the number of FLOPs for our general algorithm. From the algorithm \ref{psudo:for} we see that for each iteration $i$ of the loop we have 5 FLOPs and in algorithm \ref{psudo:back} we have 3 FLOPs for each iteration $i$ of the loop. Each loop does $n-1$ iterations, thus we have in total $(8(n-1) + 1)$ FLOPs, the $+1$ comes from the initial calculation in algortihm \ref{psudo:back}. So we have in total $8n-7$ FLOPs, for large $n$ we can ignore the cosntant terms and we are left with $8n$ FLOPs for the general algorithm. 
For very large $n$ this will come out to be $\mathcal{O}(n)$, however for comparison purposes we will use $8n$ FLOPs for the general algorithm. 

\subsection{Specialized algorithm}
In our study we have matrix on the form seen in matrix \ref{mat:tridiag_1_n} which as $2$ along the leading diagonal and $-1$ along two other diagonals. Since we have a symmetrical matrix we make it more efficient at solving the problems. If we look at our algortihms \ref{algo:forward_sub} and \ref{algo:backwards_sub} we can insert $b_i = 2$ and $a_i = c_i = -1$ for all $i$ and we get the following expressions
\begin{align}
	\tilde{b}_i &= 2 - \frac{1}{\tilde{b}_{i-1}}\label{spec:pre}\\
	\tilde{f}_i &= f^*_i + \frac{\tilde{f}_{i-1}}{\tilde{b}_{i-1}}\\
	x_i &= \frac{\tilde{f}_i + x_{i+1}}{\tilde{b}_i}
\end{align}
If we take a look at equation \ref{spec:pre}, we can show that the expressions can be rewritten on the form 
\begin{equation}
 \tilde{b}_i \equiv d_i = \frac{i+1}{i}.
\end{equation}
We see this from inserting $i = 2, 3, 4, \dots$. If we take a look at it we get 
\begin{align*}
	d_1 &= 2 \\
	d_2 &= 2 - \frac{1}{2} = \frac{3}{2}\\
	d_3 &= 2 - \frac{1}{\frac{3}{2}} = \frac{4}{3}\\
	d_4 &= 2 - \frac{1}{\frac{4}{3}} = \frac{5}{4}\\
	\vdots\\
	d_i &= \frac{i+1}{i}
\end{align*}
this can be shown to be true through, for example induction. This can be found before our algorithm reducing the number of FLOPs needed in our loops. Psuedocode for our specialized algorithm will be as follows 

\begin{algorithm}[H]\label{psudo:spec}
\SetAlgoLined
 initialization\;
 \For{i = 2, 3,\dots , n}{
	$\tilde{f}_i = f^*_i + \tilde{f}_{i-1}/d_{i-1}$\;  
 }
 $x_n = \tilde{f}_n/d_n$\;
 \For{i = n-1, n-2, \dots, 1}{
	$x_i = (\tilde{f}_i + x_{i+1})/d_i$
 }
\caption{Specialized algortihm}
\end{algorithm}

If we take a look at the number of FLOPs in our specialized algorithm we see that we have 2 FLOPs for each of the loops, running $n-1$ times. Thus our specialized algorithm has in total $4(n-1)+1$ FLOPs $=(4n-3)$ FLOPs. For large $n$ we have $4n$ FLOPs. If we compared this to the generalized algorithm, we see that it is a factor $2$. 

\subsection{LU Decomposition}
The general method for solving dense non-singular matrix is using LU decomposition. The method involves taking a matrix $\mathbf{A}$ and rewriting it into the product of a lower triangular matrix $\mathbf{L}$ and an upper triangular matrix $\mathbf{U}$. 
\begin{align*}
\mathbf{A} &= \mathbf{L}\mathbf{U}\\
\begin{bmatrix}
	a_{11} & a_{12} & a_{13} & a_{14} \\
	a_{21} &a_{22} &a_{23} &a_{24} \\
	a_{31} &a_{32} &a_{33} &a_{34}  \\
	a_{41} &a_{32} &a_{43} &a_{44} 
\end{bmatrix}
&= 
\begin{bmatrix}
1& 0 & 0 & 0 \\
 l_{21}& 1& 0 & 0\\
 l_{31}&l_{32}&1& 0\\
 l_{41}&l_{42}&l_{43}&1
\end{bmatrix}
\begin{bmatrix}
u_{11}& u_{21}&u_{31}&u_{41}\\
0 & u_{22} & u_{23}& u_{24} \\
0 & 0 & u_{33}& u_{34}\\
0 & 0 & 0 & u_{44}
\end{bmatrix}
\end{align*}

This gives us a new way of solving $\mathbf{A}\vec{x}=\vec{b}$, where the problem can be solved in two steps 
\begin{equation}
\mathbf{L}\mathbf{U}\vec{x} = \vec{b} \rightarrow \mathbf{L}\vec{w} = \vec{b}, \quad \mathbf{U}\vec{x} = \vec{w}
\end{equation}
We can solve $\mathbf{L}\vec{w} = \vec{b}$ by forward substitution and $\mathbf{U}\vec{x} = \vec{w}$ by backward substitution. 

Here we will not develop or implementing an algorithm for LU decomposition. We will use the high level library Armadillo and implement the course functions which can be found in the code folder on github, named lib.h. We are interested in the difference in runtime between the general algorithm, specialized algorithm and LU decomposition. Also included is the Armadillo function $solve$ which is adaptable and choose the quickest possible solution to the linear system. We remember that for the general algorithm the number of FLOPs needed goes like ($8n$)FLOPs and for the specialized we were able to reduce it to ($4n$)FLOPs. For LU decomposition, as we are not working with vectors, but entire matricies. The number of FLOPs needed to LU decompose a matrix is ($2/3 n^3$)FLOPs\cite{morten}. We would expect that using LU decomposition to solve the problem would result in a solver execution time going $n^2$. 

\section{Results}
\subsection{Relative error}
Here we will take a look at the relative error between the algorithms implemented. 
\subsection{CPU time}
\begin{table}[h]
\caption{Table covering runtimes from semi-cold starts for all four algorithms used. Thoose being General, Specialized, Armadillos $solve$ and the library functions from lib.hpp. Data for Armadillo $solve$ and lib.hpp are not included for $n>4$ as it is limited by memory and could not be run in this test. All times are listed in seconds, unless spesified.}
\begin{tabular}{lcccc}
\hline
 $10^n$ & General & Specialized & $solve$ & lib.hpp \\ \hline
 1 & 0.00000024 & 0.00000021 & 0.00303977 & 0.00002143 \\
 2 & 0.00000164 & 0.00000140 & 0.00008639 & 0.00325874\\
 3 & 0.00001927 & 0.00001484 & 0.00644989 & 0.98828253\\
 4 & 0.00021870 & 0.00014394 & 0.58261636 & ~42 minutes\\
 5 & 0.00223254 & 0.00147353 & n/a & n/a\\
 6 & 0.02110298 & 0.01508435 & n/a & n/a 
 \end{tabular}
\label{tab:solver_times}
\end{table}

\begin{figure}[h]
 \centering
 \includegraphics[width=0.9\linewidth]{../code/plots/plot_general.png}
 \caption{Plot of the specialized algorithm against the analytical solution. The plot shows runs for of the specialized algorithm with $n = 1, 2, 3$. End points are skipped in the plot for numerical solutions, as they do not affect the calculations and are set as $u(0) = u(n) = 0$.}
 \label{fig:special}
\end{figure}


\section{Discussion}
Discussion of the report.
\section{Conclusion}
Conclusion of the report.

%\section{Theory}
%\subsection{The Poisson Equation}
%add section on poisson equation
%>> Dirilect boundary conditions >> relation between f(x) u'' >> Check equal
%\subsection{Approximation of the Second Derivative}
%Add section on approx of second derivative
%>> Going from diff equation >> linear form Av = b >> Matrix -1,2,-1
%\subsection{Relative Error}
%Add short theory of relative error
%>> YES


%%% footnote
% rainforest\footnote{Writing out a general case will also take up more paper
% space}.

%%% Matrix with line through between last elements
% \begin{equation}
% \left[
% \begin{array}{cccc|c}
% 1 & c_1/\beta_1 & 0 & 0 & \tilde{f}_1 \\
% 0 & 1 & c_2/\beta_2 & 0  & \tilde{f}_2 \\
% 0 & 0 & 1 & c_3/\beta_3 & \tilde{f}_3 \\
% 0 & 0 & 0 & 1 & \tilde{f}_4
% \end{array}
% \right] \sim
% \left[
% \begin{array}{cccc|c}
% 1 & c_1/\beta_1 & 0 & 0 & \tilde{f}_1 \\
% 0 & 1 & c_2/\beta_2 & 0  & \tilde{f}_2 \\
% 0 & 0 & 1 & 0 & \tilde{f}_3 -\frac{c_3}{\beta_3}\tilde{f}_4 \\
% 0 & 0 & 0 & 1 & \tilde{f}_4
% \end{array}
% \right]
% \end{equation}

%%% Listing
% \lstinputlisting[language=c++, firstline=146,
% lastline=158]{../problems.cpp}

%%% LU matrix, with diag dots for general case
% \begin{equation}
% A = LU =
% \begin{bmatrix}
% 1 & 0 & 0 & \dots & 0 & 0 \\
% l_{21} & 1 & 0 & \dots & 0 & 0 \\
% l_{31} & l_{32} & 1 & \dots & 0 & 0 \\
%   &\vdots & & \ddots & \vdots  & \\
% l_{n-11} & l_{n-12} & l_{n-13} & \dots & 1 & 0 \\
% l_{n1} & l_{n2} & l_{n3} & \dots & l_{nn-1} & 1
% \end{bmatrix}
% \begin{bmatrix}
% u_{11} & u_{12} & u_{13} & \dots & u_{1n-1} & u_{1n} \\
% 0 & u_{22} & u_{23} & \dots & u_{2n-1} & u_{2n} \\
% 0 & 0 & u_{33} & \dots & u_{3n-1} & u_{3n} \\
%   &\vdots & & \ddots & \vdots  & \\
% 0 & 0 & 0 & \dots & u_{n-1n-1} & u_{n-1n} \\
% 0 & 0 & 0 & \dots & 0 & u_{nn}
% \end{bmatrix}
% \end{equation}

%%% Table
% \begin{table}[h]
% \caption{Elapsed time for increasing $n$}
% \begin{tabular}{lcc}
% \hline
% n & TDMA [s] & LU [s] \\ \hline
% 10 & 0.0000035 & 0.00106083 \\
% 100 & 0.0000116 & 0.0022319 \\
% 1000 & 0.000077892 & 0.0677764 \\
% 10000 & 0.000878769 & 21.9247 \\
% 100000 & 0.00757418 & n/a \\
% 1000000 & 0.08616075 & n/a \\
% 10000000 & 0.76534 & n/a
% \end{tabular}
% \label{tab:solver_times}
% \end{table}

%%% Figure
% \begin{figure}[h]
  % \centering
  % \includegraphics[width=0.9\linewidth]{figures/relerror.png}
  % \caption{Plot of maximum relative error as a function of step size}
  % \label{fig:relerror}
% \end{figure}

%%% Cition and bilbliography%%
% # \emph{Thomas Algorithm} \cite{thomasalgo} How to cite.
% \begin{thebibliography}{10}
  % \bibitem{thomasalgo}{Thomas, L.H. (1949), \emph{Elliptic
        % Problems in Linear Differential Equations over a
        % Network}. Watson Sci. Comput. Lab Report, Columbia University,
      % New York.}
      % \bibitem{morten}{Hjorth-Jensen, M. (2015). \emph{Computational
        % Physics - Lecture Notes 2015}. University of Oslo}
    % \bibitem{golub}{Golub, G.H., van Loan, C.F. (1996). \emph{Matrix
          % Computations} (3rd ed.), Baltimore: John Hopkins.}
% \end{thebibliography}

\bibliography{bib_proj1}
\bibliographystyle{plain}


\end{document}
